{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "6qZbUDHqsyI9",
        "outputId": "58e9545a-e0e9-47aa-f290-6311021bd044"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-83f485bd-a97c-4dea-a30f-66f9653d2f0b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-83f485bd-a97c-4dea-a30f-66f9653d2f0b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\\n  \"username\": \"anastasiasaprina\",\\n  \"key\": \"KGAT_93b03a40a8fec761f7cec4a29e6e31ef\"\\n}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZQqH2WPyX4z"
      },
      "source": [
        "тут надо подгрузить свой json с токеном кэгла\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSB0bIlG0M0I",
        "outputId": "8a58df27-f26a-404a-cd0d-475296bb880e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFAkqRAgtUbD"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFXSNtZJtWxr",
        "outputId": "31bc2ce5-8f27-40a7-c5d4-ed8bbdf82ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                           title                                                size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------------  ---------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "wardabilal/spotify-global-music-dataset-20092025              Spotify Global Music Dataset (2009–2025)          1289021  2025-11-11 09:43:05.933000          16387        389  1.0              \n",
            "neurocipher/heartdisease                                      Heart Disease                                        3491  2025-12-11 15:29:14.327000           2114         56  1.0              \n",
            "kundanbedmutha/exam-score-prediction-dataset                  Exam Score Prediction Dataset                      325454  2025-11-28 07:29:01.047000           5863        126  1.0              \n",
            "prince7489/laptop-battery-health-and-usage-dataset            Laptop Battery Health & Usage Dataset                1063  2025-12-15 15:26:25.363000            842         23  0.9411765        \n",
            "neurocipher/student-performance                               Student Performance                                 49705  2025-12-12 12:06:28.973000           1261         43  1.0              \n",
            "kundanbedmutha/student-performance-dataset                    Student Performance Dataset                        528850  2025-11-17 15:22:39.130000           2873         47  1.0              \n",
            "ajinkyachintawar/sales-and-customer-behaviour-insights        Sales & Customer Behaviour Insights                 57108  2025-12-13 09:22:09.340000           1086         32  1.0              \n",
            "rohiteng/amazon-sales-dataset                                 Amazon Sales Dataset                              4037578  2025-11-23 14:29:37.973000           6167         84  1.0              \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets list | head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNIw9OLOtZr2",
        "outputId": "59329403-a8e2-47a8-87db-df429990bb0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/data\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/data\n",
        "%cd /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H4fMAT3vjUC",
        "outputId": "bbcfdd69-a008-4e6e-c073-ffb52079bad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/dschettler8845/brats-2021-task1\n",
            "License(s): copyright-authors\n",
            "Downloading brats-2021-task1.zip to /content/data\n",
            "100% 12.3G/12.3G [00:40<00:00, 481MB/s]\n",
            "100% 12.3G/12.3G [00:40<00:00, 329MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d dschettler8845/brats-2021-task1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOz-uvU4xPUt"
      },
      "outputs": [],
      "source": [
        "!unzip -q brats-2021-task1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBOtbIwfynNa"
      },
      "outputs": [],
      "source": [
        "!tar -xf /content/data/BraTS2021_Training_Data.tar -C /content/data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWgYifNKo89z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff40a87-2a24-4f8d-a8de-159ca927897a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1342568858.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
          ]
        }
      ],
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "use_amp = (device.type == \"cuda\")\n",
        "#use_amp = False\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4owLw0fCo-Q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d39786-86c4-4b2b-de88-44c16b29a4e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patients found: 1251\n",
            "Train: 876\n",
            "Val: 250\n",
            "Test: 125\n",
            "Total: 1251\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "data_root = \"/content/data\"\n",
        "split_path = \"split_brats2021_whole_tumor.json\"\n",
        "\n",
        "\n",
        "patient_dirs = sorted(\n",
        "    [p for p in glob(os.path.join(data_root, \"*\")) if os.path.isdir(p)]\n",
        ")\n",
        "patient_ids = [os.path.basename(p) for p in patient_dirs]\n",
        "\n",
        "print(\"Patients found:\", len(patient_ids))\n",
        "\n",
        "\n",
        "if os.path.exists(split_path):\n",
        "    with open(split_path, \"r\") as f:\n",
        "        split = json.load(f)\n",
        "else:\n",
        "    rng = np.random.default_rng(42)\n",
        "    perm = rng.permutation(patient_ids)\n",
        "\n",
        "    n_total = len(perm)\n",
        "    n_test = int(0.1 * n_total)\n",
        "    n_val = int(0.2 * n_total)\n",
        "\n",
        "    test_ids = perm[:n_test].tolist()\n",
        "    val_ids = perm[n_test:n_test + n_val].tolist()\n",
        "    train_ids = perm[n_test + n_val:].tolist()\n",
        "\n",
        "    split = {\n",
        "        \"train\": train_ids,\n",
        "        \"val\": val_ids,\n",
        "        \"test\": test_ids\n",
        "    }\n",
        "\n",
        "    with open(split_path, \"w\") as f:\n",
        "        json.dump(split, f, indent=2)\n",
        "\n",
        "train_ids = split[\"train\"]\n",
        "val_ids = split[\"val\"]\n",
        "test_ids = split[\"test\"]\n",
        "\n",
        "print(\"Train:\", len(train_ids))\n",
        "print(\"Val:\", len(val_ids))\n",
        "print(\"Test:\", len(test_ids))\n",
        "print(\"Total:\", len(train_ids) + len(val_ids) + len(test_ids))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adt4GEf6IrVe"
      },
      "outputs": [],
      "source": [
        "#files.download(\"split_brats2021_whole_tumor.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL8s7d6apFTt"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Brats2DWholeTumorDataset(Dataset):\n",
        "    def __init__(self, data_root, patient_ids, bg_keep_prob=0.2, axis=2, transforms=None):\n",
        "        self.data_root = data_root\n",
        "        self.patient_ids = patient_ids\n",
        "        self.bg_keep_prob = bg_keep_prob\n",
        "        self.axis = axis\n",
        "        self.transforms = transforms\n",
        "\n",
        "\n",
        "        self.items = []\n",
        "        for pid in self.patient_ids:\n",
        "            pdir = os.path.join(self.data_root, pid)\n",
        "            flair = glob(os.path.join(pdir, \"*flair*.nii.gz\")) + glob(os.path.join(pdir, \"*FLAIR*.nii.gz\"))\n",
        "            seg = glob(os.path.join(pdir, \"*seg*.nii.gz\")) + glob(os.path.join(pdir, \"*SEG*.nii.gz\"))\n",
        "            if len(flair) == 0 or len(seg) == 0:\n",
        "                continue\n",
        "            self.items.append({\"pid\": pid, \"flair\": flair[0], \"seg\": seg[0]})\n",
        "\n",
        "        if len(self.items) == 0:\n",
        "            raise RuntimeError(\"No valid patients found. Check data_root and file naming.\")\n",
        "\n",
        "\n",
        "        self.epoch_length = len(self.items) * 16\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.epoch_length\n",
        "\n",
        "    @staticmethod\n",
        "    def _zscore(x, eps=1e-6):\n",
        "        m = x.mean()\n",
        "        s = x.std()\n",
        "        return (x - m) / (s + eps)\n",
        "\n",
        "    def _get_slice(self, vol, idx):\n",
        "        if self.axis == 0:\n",
        "            return vol[idx, :, :]\n",
        "        if self.axis == 1:\n",
        "            return vol[:, idx, :]\n",
        "        return vol[:, :, idx]\n",
        "\n",
        "    def __getitem__(self, _):\n",
        "\n",
        "        item = random.choice(self.items)\n",
        "\n",
        "        flair_3d = nib.load(item[\"flair\"]).get_fdata().astype(np.float32)\n",
        "        seg_3d = nib.load(item[\"seg\"]).get_fdata().astype(np.int16)\n",
        "\n",
        "\n",
        "        flair_3d = self._zscore(flair_3d)\n",
        "        flair_3d = np.nan_to_num(flair_3d, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "\n",
        "\n",
        "        mask_3d = (seg_3d > 0).astype(np.float32)\n",
        "\n",
        "\n",
        "        n_slices = flair_3d.shape[self.axis]\n",
        "\n",
        "\n",
        "        chosen = None\n",
        "        for _try in range(12):\n",
        "            idx = random.randrange(n_slices)\n",
        "            m = self._get_slice(mask_3d, idx)\n",
        "            if m.sum() > 0:\n",
        "                chosen = idx\n",
        "                break\n",
        "\n",
        "        if chosen is None:\n",
        "\n",
        "            if random.random() < self.bg_keep_prob:\n",
        "                chosen = random.randrange(n_slices)\n",
        "            else:\n",
        "\n",
        "                while True:\n",
        "                    idx = random.randrange(n_slices)\n",
        "                    m = self._get_slice(mask_3d, idx)\n",
        "                    if m.sum() > 0:\n",
        "                        chosen = idx\n",
        "                        break\n",
        "\n",
        "        img2d = self._get_slice(flair_3d, chosen)\n",
        "        msk2d = self._get_slice(mask_3d, chosen)\n",
        "\n",
        "\n",
        "        img2d = img2d[None, ...]\n",
        "        msk2d = msk2d[None, ...]\n",
        "\n",
        "        sample = {\"image\": img2d, \"mask\": msk2d, \"pid\": item[\"pid\"], \"slice\": chosen}\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            sample = self.transforms(sample)\n",
        "\n",
        "\n",
        "        image = torch.from_numpy(sample[\"image\"]).float()\n",
        "        mask = torch.from_numpy(sample[\"mask\"]).float()\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b22M0aupH8d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds = Brats2DWholeTumorDataset(\n",
        "    data_root=data_root,\n",
        "    patient_ids=train_ids,\n",
        "    bg_keep_prob=0.15,\n",
        "    axis=2,\n",
        "    transforms=None\n",
        ")\n",
        "\n",
        "val_ds = Brats2DWholeTumorDataset(\n",
        "    data_root=data_root,\n",
        "    patient_ids=val_ids,\n",
        "    bg_keep_prob=1.0,\n",
        "    axis=2,\n",
        "    transforms=None\n",
        ")\n",
        "\n",
        "test_ds = Brats2DWholeTumorDataset(\n",
        "    data_root=data_root,\n",
        "    patient_ids=test_ids,\n",
        "    bg_keep_prob=1.0,\n",
        "    axis=2,\n",
        "    transforms=None\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDwALd5nzq0J",
        "outputId": "41e73ee2-f173-4d67-df3a-17b018fdb562"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting monai\n",
            "  Downloading monai-1.5.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.24 in /usr/local/lib/python3.12/dist-packages (from monai) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from monai) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.1->monai) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.1->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.1->monai) (3.0.3)\n",
            "Downloading monai-1.5.1-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: monai\n",
            "Successfully installed monai-1.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82eTRFWOpKRr"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from monai.networks.nets import UNet\n",
        "from monai.losses import DiceLoss\n",
        "from monai.metrics import DiceMetric\n",
        "\n",
        "model = UNet(\n",
        "    spatial_dims=2,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    channels=(32, 64, 128, 256, 512),\n",
        "    strides=(2, 2, 2, 2),\n",
        "    num_res_units=2,\n",
        ").to(device)\n",
        "\n",
        "loss_fn = DiceLoss(sigmoid=True)\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsS7VFvjz4K_"
      },
      "outputs": [],
      "source": [
        "save_path = \"/content/drive/MyDrive/brats/unet2d_flair_whole_tumor_best.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmbMDGsVpPBG",
        "outputId": "570c1123-756c-4aed-d3b2-1a97c2be1c4a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/6: 100%|██████████| 876/876 [33:58<00:00,  2.33s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train_loss=0.2921 | val_loss=0.2701 | val_dice=0.7410\n",
            "Saved best model checkpoint: Dice=0.7410\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/6: 100%|██████████| 876/876 [34:31<00:00,  2.36s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: train_loss=0.2312 | val_loss=0.2432 | val_dice=0.7633\n",
            "Saved best model checkpoint: Dice=0.7633\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/6: 100%|██████████| 876/876 [34:35<00:00,  2.37s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: train_loss=0.2180 | val_loss=0.2421 | val_dice=0.7652\n",
            "Saved best model checkpoint: Dice=0.7652\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/6: 100%|██████████| 876/876 [34:20<00:00,  2.35s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: train_loss=0.2076 | val_loss=0.2302 | val_dice=0.7770\n",
            "Saved best model checkpoint: Dice=0.7770\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/6: 100%|██████████| 876/876 [34:35<00:00,  2.37s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: train_loss=0.2007 | val_loss=0.2246 | val_dice=0.7817\n",
            "Saved best model checkpoint: Dice=0.7817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/6:  50%|████▉     | 436/876 [17:11<17:10,  2.34s/it]"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from torch import amp\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    dice_metric.reset()\n",
        "    val_loss = 0.0\n",
        "    n = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            masks = masks.to(device, non_blocking=True)\n",
        "\n",
        "            with amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=use_amp):\n",
        "                logits = model(images)\n",
        "                loss = loss_fn(logits, masks)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            n += images.size(0)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()\n",
        "            dice_metric(y_pred=preds, y=masks)\n",
        "\n",
        "    mean_dice = dice_metric.aggregate().item()\n",
        "    return val_loss / max(n, 1), mean_dice\n",
        "\n",
        "\n",
        "best_dice = -1.0\n",
        "epochs = 6\n",
        "save_path = save_path\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        masks = masks.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=use_amp):\n",
        "            logits = model(images)\n",
        "            loss = loss_fn(logits, masks)\n",
        "\n",
        "        \"\"\"\n",
        "        if not torch.isfinite(loss):\n",
        "            print(\"Non-finite loss!\")\n",
        "            print(\"images finite:\", torch.isfinite(images).all().item())\n",
        "            print(\"masks finite:\", torch.isfinite(masks).all().item())\n",
        "            print(\"images min/max:\", images.min().item(), images.max().item())\n",
        "            print(\"masks unique:\", torch.unique(masks).cpu().numpy()[:10])\n",
        "            print(\"logits finite:\", torch.isfinite(logits).all().item())\n",
        "            print(\"logits min/max:\", logits.min().item(), logits.max().item())\n",
        "            break\n",
        "        \"\"\"\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        n += images.size(0)\n",
        "\n",
        "    train_loss /= max(n, 1)\n",
        "    val_loss, val_dice = evaluate(model, val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_dice={val_dice:.4f}\")\n",
        "\n",
        "    if val_dice > best_dice:\n",
        "        best_dice = val_dice\n",
        "\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"scaler_state_dict\": scaler.state_dict() if use_amp else None,\n",
        "            \"val_dice\": val_dice,\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, save_path)\n",
        "        print(f\"Saved best model checkpoint: Dice={best_dice:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# load best checkpoint\n",
        "ckpt = torch.load(save_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "\n",
        "\n",
        "test_loss, test_dice = evaluate(model, test_loader)\n",
        "print(f\"TEST: loss={test_loss:.4f} | dice={test_dice:.4f} (best val dice={ckpt['val_dice']:.4f} at epoch {ckpt['epoch']})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXqKEbEJMUJf"
      },
      "outputs": [],
      "source": [
        "#rm -f unet2d_flair_whole_tumor_best.pt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = \"/content/drive/MyDrive/brats/unet2d_flair_whole_tumor_best.pt\"\n"
      ],
      "metadata": {
        "id": "g7h1RJc_-bgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from monai.networks.nets import UNet\n",
        "from monai.losses import DiceLoss\n",
        "from monai.metrics import DiceMetric\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = UNet(\n",
        "    spatial_dims=2,\n",
        "    in_channels=1,\n",
        "    out_channels=1,\n",
        "    channels=(32, 64, 128, 256, 512),\n",
        "    strides=(2, 2, 2, 2),\n",
        "    num_res_units=2,\n",
        ").to(device)\n",
        "\n",
        "loss_fn = DiceLoss(sigmoid=True)\n",
        "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "use_amp = torch.cuda.is_available()\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzgPSZtrB57R",
        "outputId": "e9d18f9d-f7d9-43c1-f8a6-4619ec7ae9ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2122120143.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = ckpt_path\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
        "\n",
        "if use_amp and ckpt.get(\"scaler_state_dict\") is not None:\n",
        "    scaler.load_state_dict(ckpt[\"scaler_state_dict\"])\n",
        "\n",
        "start_epoch = ckpt[\"epoch\"] + 1\n",
        "best_dice = ckpt.get(\"val_dice\", -1.0)\n",
        "\n",
        "print(\"Resumed from epoch:\", start_epoch)\n",
        "print(\"Best val dice so far:\", best_dice)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p00gUnqDIWz",
        "outputId": "6e3e36a5-62b7-4efc-a8b8-ba8711b176d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumed from epoch: 7\n",
            "Best val dice so far: 0.7866259217262268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torch import amp\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    dice_metric.reset()\n",
        "    val_loss = 0.0\n",
        "    n = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            masks = masks.to(device, non_blocking=True)\n",
        "\n",
        "            with amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=use_amp):\n",
        "                logits = model(images)\n",
        "                loss = loss_fn(logits, masks)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            n += images.size(0)\n",
        "\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()\n",
        "            dice_metric(y_pred=preds, y=masks)\n",
        "\n",
        "    mean_dice = dice_metric.aggregate().item()\n",
        "    return val_loss / max(n, 1), mean_dice\n",
        "\n",
        "\n",
        "best_dice = -1.0\n",
        "epochs = 8\n",
        "save_path = save_path\n",
        "\n",
        "for epoch in range(start_epoch, epochs + 1):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        masks = masks.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=use_amp):\n",
        "            logits = model(images)\n",
        "            loss = loss_fn(logits, masks)\n",
        "\n",
        "        \"\"\"\n",
        "        if not torch.isfinite(loss):\n",
        "            print(\"Non-finite loss!\")\n",
        "            print(\"images finite:\", torch.isfinite(images).all().item())\n",
        "            print(\"masks finite:\", torch.isfinite(masks).all().item())\n",
        "            print(\"images min/max:\", images.min().item(), images.max().item())\n",
        "            print(\"masks unique:\", torch.unique(masks).cpu().numpy()[:10])\n",
        "            print(\"logits finite:\", torch.isfinite(logits).all().item())\n",
        "            print(\"logits min/max:\", logits.min().item(), logits.max().item())\n",
        "            break\n",
        "        \"\"\"\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        n += images.size(0)\n",
        "\n",
        "    train_loss /= max(n, 1)\n",
        "    val_loss, val_dice = evaluate(model, val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_dice={val_dice:.4f}\")\n",
        "\n",
        "    if val_dice > best_dice:\n",
        "        best_dice = val_dice\n",
        "\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"scaler_state_dict\": scaler.state_dict() if use_amp else None,\n",
        "            \"val_dice\": val_dice,\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, save_path)\n",
        "        print(f\"Saved best model checkpoint: Dice={best_dice:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# load best checkpoint\n",
        "ckpt = torch.load(save_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "\n",
        "\n",
        "test_loss, test_dice = evaluate(model, test_loader)\n",
        "print(f\"TEST: loss={test_loss:.4f} | dice={test_dice:.4f} (best val dice={ckpt['val_dice']:.4f} at epoch {ckpt['epoch']})\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwdF7ObRDYZ7",
        "outputId": "7ea4aef4-09e1-4caf-8f02-362b1bd34391"
      },
      "execution_count": 31,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/8: 100%|██████████| 876/876 [34:51<00:00,  2.39s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: train_loss=0.1896 | val_loss=0.2075 | val_dice=0.7982\n",
            "Saved best model checkpoint: Dice=0.7982\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/8: 100%|██████████| 876/876 [33:12<00:00,  2.27s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: train_loss=0.1864 | val_loss=0.2156 | val_dice=0.7914\n",
            "TEST: loss=0.2017 | dice=0.8011 (best val dice=0.7982 at epoch 7)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}